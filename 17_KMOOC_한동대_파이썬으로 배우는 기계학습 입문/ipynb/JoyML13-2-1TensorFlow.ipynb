{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬으로 배우는 기계학습\n",
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 13-2 강: 기계학습 오픈 프레임워크 - Tensorflow\n",
    "\n",
    "\n",
    "## 학습 목표\n",
    "- 기계학습을 위한 오픈 프레임워크는 무엇이 있는지 알아본다.\n",
    "- TensorFlow, Keras, PyTorch가 무엇인지 이해한다.\n",
    "- CNN을 이용한 MNIST 데이터를 3가지 프레임워크로 학습하는 것을 이해한다. \n",
    "\n",
    "## 학습 내용\n",
    "- 신경망 구현을 위한 패키지\n",
    "- Tensorflow 란?\n",
    "- Tensorflow 환경 구축\n",
    "- Tensorflow 를 이용한 MNIST 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 신경망 구현을 위한 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 우리는 인공신경망이 어떤 원리로 동작하는지에 대해 살펴보았고, 각 단계마다 직접 그것을 구현했습니다. 지난 강의에서는 MNIST 데이터셋에 적용을 해보았습니다. 혹시 여러분이 코딩하는데 어려움이 있었을지도 모르겠습니다. 인공 신경망의 은닉층이 많아지거나, 데이터의 특성에 따라 신경망에 새로운 구조를 추가시켜야 할 경우도 있습니다. 이러할 때마다 우리가 앞서 구현했던 NeuralNetwork 클래스에 새로운 기능을 추가하기에는 많은 시간과 노력이 따를 것입니다.\n",
    "\n",
    "다행히도, 우리는 신경망을 사용할때마다 이와같이 부가적인 코딩을 하지 않아도 됩니다. 이러한 목적으로 개발된 패키지가 많이 있으며, 그 중 유명한 몇가지를 소개한다면 다음과 같습니다:\n",
    "\n",
    "- [Tensorflow](https://www.tensorflow.org/)\n",
    "- [Keras](https://keras.io/)\n",
    "- [Pytorch](https://pytorch.org/)\n",
    "- [Caffe](http://caffe.berkeleyvision.org/)\n",
    "- [Theano](http://deeplearning.net/software/theano/)\n",
    "- [Scikit-learn](http://scikit-learn.org/)\n",
    "\n",
    "이번 강의에서는 Tensorflow 를 소개하겠습니다. Tensorflow 를 사용한다면 간단하게 신경망을 구성할 수 있습니다. 데이터를 읽어드린 다음, 신경망을 구성하고 학습시켜 결과를 예측하는 것을 보여드리겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensorflow 란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/tensorflow.png?raw=true\" width=\"200\">\n",
    "<center>그림 1: Tensorflow [출처](https://tensorflow.org/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow는 구글이 오픈 소스로 개발하여, 이 분야의 주류가 되어가고 있습니다. 핵심 기술은 C++로 작성되었지만,  프론트 엔드 부분은 파이썬으로 작성되어서, 파이썬으로 쉽게 접근이 가능합니다. \n",
    "Tensorflow를 이용해서 이미지, 음성, 비디오 등 다양하고 많은 자료를 기계학습 할 수 있으며, GPU(Graphics Processing Unit)와 같은 하드웨어를 사용합니다. <br>\n",
    "\n",
    "참고로, 수학에서 0차원의 수를 스칼라$^{scalar}$, 1차원 배열을 벡터$^{vector}$, 2차원 배열을 행렬$^{matrix}$라고 부릅니다.  이와 같이 스칼라, 벡터와 행렬을 일반화한 것을 텐서$^{tensor}$라고 합니다. \n",
    "TensorBoard 라는 훌륭한 시각화 툴을 제공하고 있어 학습 과정을 모니터링 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensorflow 환경 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 를 사용하기 위해서는 Python 3.5 이상 버전이 설치되어야 합니다. 하지만, 우리는 Python이 포함된 아나콘다를 이용해서 Tensorflow를 설치할 것이기 때문에, Python을 따로 설치하지는 않아도 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Anaconda 설치\n",
    "\n",
    "Anaconda3 를 설치하세요. 다음 링크를 통해 설치하시면 됩니다: [링크](https://www.continuum.io/downloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tensorflow 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OS X or Linux\n",
    "\n",
    "아래의 명령어를 사용해서 Tensorflow 를 설치하세요.\n",
    "\n",
    "```\n",
    "conda create -n tensorflow python=3.5\n",
    "source activate tensorflow\n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "#### Windows\n",
    "\n",
    "cmd 혹은 Anaconda 쉘에서 아래의 명령어를 사용해서 Tensorflow 를 설치하세요.\n",
    "\n",
    "```\n",
    "conda create -n tensorflow python=3.5\n",
    "activate tensorflow\n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "Tensorflow 가 정상적으로 설치되었는지 확인하기 위해 아래의 코드를 실행해보세요. tensorflow 라이브러리를 import 할 때에 문제가 생기지 않는다면, 정상적으로 설치된 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 설치중에 어려움이 있다면, 아래 링크를 참고하도록 합니다.\n",
    "\n",
    "- [Anaconda 설치](https://www.continuum.io/downloads)\n",
    "- [tensorflow 설치](https://www.tensorflow.org/install/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tensorflow 를 이용한 MNIST 데이터 분석\n",
    "\n",
    "이제 Tensorflow 를 사용할 환경이 구축되었습니다. Tensorflow 를 이요해서 MNIST 데이터를 분석해보도록 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 MNIST 데이터 읽어오기\n",
    "\n",
    "Tensorflow 에서는 [Yann LeCun 교수의 웹 사이트](http://yann.lecun.com/exdb/mnist/)에 있는 MNIST 데이터셋을 불러올 수 있습니다. 먼저 MNIST 데이터를 읽어오겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다운로드된 데이터는 55,000개의 학습 데이터$^{Train\\ data}$와 10,000개의 평가 데이터$^{Test\\ data}$, 그리고 5,000개의 검증 데이터$^{Validation\\ data}$ 이렇게 세 부분으로 나뉩니다.\n",
    "각각의 데이터는 mnist.train, mnist.test, mnist.validation으로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mnist.train.images.shape: {}\\nmnist.train.labels.shape: {}\\nmnist.test.images.shape: {}\\nmnist.test.labels.shape: {}\".\n",
    "      format(mnist.train.images.shape, mnist.train.labels.shape, mnist.test.images.shape, mnist.test.labels.shape))\n",
    "print(\"mnist.validation.images.shape: {}\\nmnist.validation.labels.shape: {}\".\n",
    "     format(mnist.validation.images.shape, mnist.validation.labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape: {}\".format(mnist.train.images.shape))\n",
    "print(\"y shape: {}\".format(mnist.train.labels.shape))\n",
    "print(\"X test shape: {}\".format(mnist.test.images.shape))\n",
    "print(\"y test shape: {}\".format(mnist.test.labels.shape))\n",
    "print(\"X validation shape: {}\".format(mnist.validation.images.shape))\n",
    "print(\"y validation shape: {}\".format(mnist.validation.labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 신경망 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow를 사용하기 위해서는 이를 임포트 해야 합니다.\n",
    "\n",
    "Tensorflow에서 연산에 사용할 변수들을 만들어야 합니다. Tensorflow에서는 다음과 같이 `placeholder`를 이용하여 만들어 줍니다. `placeholder`는 Tensorflow에서 연산을 실행할 때 값을 입력할 공간(자리)입니다. MNIST 데이터에서는 784차원의 벡터를 넣어주기 때문에 `[None, 784]`의 형태를 지정해 줍니다. 이 공간에는 부동소수점을 가지고 있는 2차원 텐서$^{Tensor}$를 표현합니다. 참고로, None은 어떤 길이도 될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망이 학습을 하려면 입력만 있으면 되나요? 가중치와 편향이 필요하겠죠?<br>\n",
    "Tensorflow에서는 연산 중간에 값을 수정할 수 있도록 `Variable`이라고 불리는 기능을 제공해 줍니다. 지금은 `w`와 `b`를 0으로 초기화를 합시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "dropout_ratio = tf.placeholder(tf.float32)\n",
    "\n",
    "x_train = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_train, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, dropout_ratio)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_hat = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y_hat = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "'''\n",
    "CE = tf.reduce_mean(\n",
    "        -tf.reduce_sum(y * tf.log(y_hat), reduction_indices=[1])\n",
    "     )\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(CE)\n",
    "correct_prediction = tf.equal(tf.argmax(y_hat,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(10):\n",
    "        batch = mnist.train.next_batch(32)\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = accuracy.eval(session=sess, feed_dict={\n",
    "                x:batch[0], y: batch[1], dropout_ratio: 1.0 })\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y: batch[1], dropout_ratio: 0.2})\n",
    "    \n",
    "    acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels, dropout_ratio: 1.0})\n",
    "    print(\"Test accuracy: {}\".format(acc))\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./board/joy_mnist/1\")\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 신경망을 만드는데 필요한 재료들이 모두 준비가 되었죠? 이렇게 준비된 재료들로 모델을 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 학습 시킬텐데요. 우리는 학습시킬 모델의 성능을 확인할 비용$^{cost}$ 혹은 손실$^{loss}$라고 부르는 값을 정해줄 것입니다. 모델의 손실을 정의하기 위해 자주 사용되는 함수로는 교차 엔트로피$^{Cross\\ Entropy}$가 있습니다. 교차 엔트로피$^{Cross\\ Entropy}$를 구현하기 위해서는 정답$^{label}$을 넣기 위한 새로운 `placeholder`를 추가해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리는 교차 엔트로피$^{Cross\\ Entropy}$를 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE = tf.reduce_mean(\n",
    "        -tf.reduce_sum(y * tf.log(y_hat), reduction_indices=[1])\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 손실 함수까지 만들었으니 기계학습의 꽃인 역전파$^{Backpropagation}$ 알고리즘을 정의해보겠습니다. 역전파$^{Backpropagation}$ 알고리즘에서는 교차 엔트로피$^{Cross \\ Entropy}$를 최소화합니다. 학습율은 0.5로 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(CE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 이제 모델을 학습할 준비가 다 되었습니다. 모델을 만들었으니, 학습을 하기 전에 모든 변수들을 초기화 하도록 하겠습니다. 그 후에 `Session`에서 모델을 실행시키고, 1000번 학습을 하도록 해보겠습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(1000):\n",
    "    batch_x, batch_y = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch_x, batch_y = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 모델 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "방금 우리가 만든 모델의 성능은 어느 정도일까요? 확인해 보도록 하겠습니다. `tf.argmax(y_hat,1)`은 모델이 생각하기에 데이터에 가장 적합하다고 판단한 예측을 말하고, `tf.argmax(y,1)`은 실제 레이블 입니다. 이를 `tf.equal`을 사용하여 모델이 예측한 값이 실제 레이블과 맞는지 비교해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_hat,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"Test accuracy: {}\".format(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료\n",
    "\n",
    "- [Tensorflow 공식 홈페이지](https://www.tensorflow.org/versions/r1.0/get_started/mnist/beginners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "Rejoice in the Lord always. I will say it again: Rejoice! (Ph4:4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
