{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬으로 배우는 기계학습\n",
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 13-2 강: 기계학습 오픈 프레임워크 - Keras\n",
    "\n",
    "\n",
    "## 학습 목표\n",
    "- 기계학습을 위한 오픈 프레임워크는 무엇이 있는지 알아본다.\n",
    "- TensorFlow, Keras, PyTorch가 무엇인지 이해한다.\n",
    "- CNN을 이용한 MNIST 데이터를 3가지 프레임워크로 학습하는 것을 이해한다. \n",
    "\n",
    "## 학습 내용\n",
    "- 신경망 구현을 위한 패키지\n",
    "- Keras 란?\n",
    "- Keras 환경 구축\n",
    "- Keras 를 이용한 MNIST 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 신경망 구현을 위한 패키지\n",
    "이번 강의에서는 Keras 를 소개하겠습니다. Keras 를 사용한다면 간단하게 신경망을 구성할 수 있습니다. 데이터를 읽어드린 다음, 신경망을 구성하고 학습시켜 결과를 예측하는 것을 보여드리겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keras 란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/keras.png?raw=true\" width=\"300\">\n",
    "<center>그림 1: Keras [출처](https://keras.io/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 는 Python 기반으로 쓰여졌으며 TensorFlow 혹은 CNTK, Theano 위에서 실행되는 신경망 API입니다. 문법을 간단하게 그리고 직관적으로 만들어 쉽게 구현할 수 있도록 만들어졌습니다. Keras 를 사용해서 Convolutional Neural Network 와 Recurrent Neural Network 도 구현할 수 있으며 두가지 딥러닝 모델을 섞어서 구현하는 것도 가능합니다. 또한 하나의 코드로 CPU 와 GPU 에서 모두 동작합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras 환경 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 를 설치하기에 앞서 backend 엔진으로 사용할 TensorFlow 혹은 Theano, CNTK 중에 하나를 골라서 설치해야 합니다. 이번 강의에서 우리는 tensorflow 를 설치할 것입니다. \n",
    "\n",
    "지금까지의 경험으로보면, 가장 쉽게 Keras 를 설치하는 방법은 다음과 같습니다. Anaconda 를 우선 설치하고, Conda 명령어로 tensorflow 를 설치한 후에 Keras 를 설치하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Anaconda 설치\n",
    "\n",
    "Anaconda3 를 설치하세요. 다음 링크를 통해 설치하시면 됩니다: [링크](https://www.continuum.io/downloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tensorflow 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OS X or Linux\n",
    "\n",
    "아래의 명령어를 사용해서 Tensorflow 를 설치하세요.\n",
    "\n",
    "```\n",
    "conda create -n tensorflow python=3.5\n",
    "source activate tensorflow\n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "#### Windows\n",
    "\n",
    "cmd 혹은 Anaconda 쉘에서 아래의 명령어를 사용해서 Tensorflow 를 설치하세요.\n",
    "\n",
    "```\n",
    "conda create -n tensorflow python=3.5\n",
    "activate tensorflow\n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "Tensorflow 가 정상적으로 설치되었는지 확인하기 위해 아래의 코드를 실행해보세요. tensorflow 라이브러리를 import 할 때에 문제가 생기지 않는다면, 정상적으로 설치된 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Keras 설치\n",
    "\n",
    "Tensorflow 까지 설치했다면, Keras 설치하는건 간단합니다. 콘솔창에서 다음 명령을 실행하면 됩니다.\n",
    "\n",
    "```sudo pip install keras```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "금방 설치되었죠? 여기서 중요한 부분이 있습니다. Keras 를 사용하려면, 항상 콘솔창에서 tensorflow 를 사용할 수 있는 환경을 활성화시켜주어야 합니다. `source activate tensorflow` 를 실행한 다음에, tensorflow 를 backend 로 해서 파이썬 스크립트를 실행하세요. Jupyter Notebook을 사용한다면, Jupyter notebook 을 실행하기 전에 콘솔에서 `source activate tensorflow` 를 해주고, 그 다음에 notebook을 열어야 합니다.\n",
    "\n",
    "다음 명령어를 실행해보세요. 정상적으로 설치되었다면 'Using TensorFlow backend.' 라는 문구가 보일겁니다. Keras 를 사용하기 위한 문법은 뒤에서 자세하게 설명하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 설치중에 어려움이 있다면, 아래 링크를 참고하도록 합니다.\n",
    "\n",
    "- [Anaconda 설치](https://www.continuum.io/downloads)\n",
    "- [tensorflow 설치](https://www.tensorflow.org/install/)\n",
    "- [Keras 설치](https://keras.io/#installation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras 를 이용한 MNIST 데이터 분석\n",
    "\n",
    "이제 Keras 를 사용할 환경이 구축되었습니다. Keras 를 이요해서 MNIST 데이터를 분석해보도록 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 MNIST 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 에는 datasets 라는 모듈이 있으며, 사람들이 많이 사용하는 데이터를 쉽게 사용할 수 있도록 만들어져 있습니다. MNIST 데이터를 읽어오기 위해서 아래와 같이 두 줄이면 충분합니다. keras.datasets 에 있는 mnist 클래스에서 load_data 메소드를 호출하면 MNIST 데이터를 임의로 섞어서 학습시킬 때 사용할 데이터와 테스트할 때 사용할 데이터로 나눠줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나뉘어진 데이터 세트의 형상은 아래와 같습니다. 학습할 데이터에는 60,000개의 입력값이, 테스트할 `X` 데이터에는 10,000개의 입력값이 저장되어있는 것을 볼 수 있습니다. 각각의 입력값은 28 x 28 의 크기로 0 ~ 255 범위에 있는 숫자가 저장됩니다. `y` 에는 해당 입력값의 실제 숫자 0 ~ 9가 저장됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train.shape: {}\\ny_train.shape: {}\\nX_test.shape: {}\\ny_test.shape:{}\\n\".\n",
    "      format(x_train.shape, y_train.shape, x_test.shape, y_test.shape))\n",
    "print(\"X_train: minimum value={}, maximum value={}\".format(x_train.min(), x_train.max()))\n",
    "print(\"X_test: minimum value={}, maximum value={}\".format(x_test.min(), x_test.max()))\n",
    "print(\"y_train: minimum value={}, maximum value={}\".format(y_train.min(), y_train.max()))\n",
    "print(\"y_test: minimum value={}, maximum value={}\".format(y_test.min(), y_test.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 모든 feature의 값을 0 ~ 1 사이로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계학습을 하게되면 일반적으로 모든 feature를 0 에서 1 사이의 값으로 맞추어서 모델을 학습하곤 합니다. 여러 이유가 있겠지만, (1) 각 입력값의 특정 feature 가 큰 범위로 변할 경우 다른 feature의 영향력이 모델링 하는 단계에서 무시될 수 있기도 하며 (2) 비용함수를 계산하는 과정에서 값이 너무 크게되는 경우도 발생합니다.\n",
    "\n",
    "MNIST 데이터는 각 feature들이 왼쪽 위에서부터 오른쪽 아래까지의 pixel 값들이기 때문에, 모든 feature들의 최소값은 0이고 최대값은 255 입니다. 따라서 모든 feature들의 값을 255로 나눠주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음 정수로 저장된 `y` 값을 one-hot 으로 바꿔주겠습니다. `keras.utils` 에 있는 `np_utils` 메소드를 사용해서 쉽게 해결할 수 있습니다. 아래의 간단한 예제는 정수로 저장되어있는 `one_to_ten` 넘파이 어레이를 one-hot 으로 바꿔줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "one_to_ten = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "one_to_ten = np_utils.to_categorical(one_to_ten, 10)\n",
    "print(one_to_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 one-hot 인코딩\n",
    "\n",
    "이제 `y_train` 과 `y_test` 를 one-hot 인코딩 하겠습니다. `y_train` 의 처음 5개 레이블을 `[5 0 4 1 9]` 입니다. one-hot 인코딩이 정말 간단하죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous five labels in y_train: [5 0 4 1 9]\n",
      "One-hot encoded labels of y_train: \n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"previous five labels in y_train: {}\".format(y_train[:5]))\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "print('One-hot encoded labels of y_train: \\n{}'.format(y_train[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 신경망 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 신경망을 구축합니다. 가장 간단한 종류의 신경망힌 `Sequential` 을 구축하겠습니다. `Sequential` 은 우리가 지금까지 배운 내용과 같이 단계적으로 층을 쌓아가는 것입니다. `Sequential()` 로 모델을 정의해주고, 층을 더할 때는 `add` 메소드를 사용하면 됩니다. 처음 입력값의 feature들이 몇개가 되는지 `model.add(Flatten(input_shape=X_train.shape[1:]))` 를 통해 알려주며, `Dense` 층 즉, 완전이 연결된 신경망을 더해줍니다. 1개의 은닉층에 총 512개의 뉴런을 사용하고 활성화함수로트 `relu` 를 사용하겠습니다.\n",
    "\n",
    "Dropout 은 처음 보는 개념일 수도 있는데, 간단히 설명하자면 특정 연결망이 너무 강해지는것을 방지하기 위한 용도입니다. 매번 학습을 시킬 때마다 임의로 20% 정도씩의 신경망을 학습하지 않도록 하겠습니다.\n",
    "\n",
    "마지막 층은 10개의 뉴런을 사용하며, 각각의 뉴런은 0부터 9까지의 숫자를 예측하는 역할을 합니다. 활성화함수로 `softmax` 를 사용하여서, 특정 입력값이 들어왔을 때, 마지막 층의 뉴런이 출력해내는 값의 합이 1이 되게끔 합니다. 즉, 해당 입력값에 대해 그것이 0일 확률 부터 9일 확률까지를 보여주는 것으로 해석할 수도 있습니다.\n",
    "\n",
    "`model.summary()` 를 해주면, 우리가 구축한 모델을 요약해서 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 컴파일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 이제 신경망을 구축했으니 모델을 컴파일하면 되겠군요. 이 단계에서는 어떤 비용함수를 쓸 것인지 (loss), 어떠한 최적화기를 사용할 것인지 (optimizer), 무엇을 기준으로 학습 할 것인지 (metrics) 를 설정해줍니다. 아래의 조합은 분류를 할 때에 일반적으로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 학습시키는 단계입니다. `keras.callbacks` 에 있는 `ModelCheckpoint` 메소드를 사용하면 학습 결과가 좋아질 때 마다 그때의 가중치를 지정한 `filepath` 에 저장할 수 있습니다.\n",
    "\n",
    "`fit` 메소드를 사용하면 학습을 시작합니다. training 데이터를 학습할 때 사용 할 데이터와 검증 집합 (validation set) 으로 나누어서 각 `epoch` 마다 검증 집합의 loss 가 증가한다면 학습이 되고있다고 볼 수 있습니다. 그리고 각 `epoch` 마다 한번에 몇 묶음의 입력값을 사용하여 동시에 가중치를 변화시킬 것인지에 해당하는 `batch_size` 를 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 141s 2ms/step - loss: 0.2003 - acc: 0.9390 - val_loss: 0.0462 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04622, saving model to mnist.model.best.hdf5\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 158s 3ms/step - loss: 0.0489 - acc: 0.9847 - val_loss: 0.0535 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.04622\n",
      "Epoch 3/10\n",
      " 2304/60000 [>.............................] - ETA: 2:29 - loss: 0.0363 - acc: 0.9905"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='mnist.model.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "         callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 분류 정확도 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검증 집합에서 최고의 성능을 보여주는 모델의 가중치가 `mnist.model.best.hdf5` 에 저장되어 있습니다. 이 가중치를 불러온 후에, 테스트 세트를 예측해봅니다.\n",
    "\n",
    "`evaluate` 메소드를 사용해서, 테스트 세트에 있는 입력값들의 레이블을 예측합니다. `evaluate` 메소드는 두 가지 인자를 반환하는데, `loss` 값과 `metric` 에 지정한 값입니다. `loss` 는 학습시킬 때 사용한 것이고, `metric` 은 우리가 앞서 지정한 정확도를 의미합니다. \n",
    "\n",
    "코드를 실행할 때마다 accuracy 는 차이가 있겠지만 위와 같은 인자들로 신경망을 학습시켰을 때에 98% 전후의 정확도를 보여줍니다. 즉, 0에서 9까지 적혀있는 28 x 28 사이즈의 입력값이 들어왔을 때에, 98% 정확도로 어떤 숫자인지 알아내는 신경망을 학습시킨 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Udacity 강의 'Artificial Intelligence Nanodegree - Convolutional Neural Networks' [aind2-cnn Jupyter Notebook 파일](https://github.com/udacity/aind2-cnn/blob/master/mnist-mlp/mnist_mlp.ipynb)\n",
    "- Keras Documentation https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "Rejoice in the Lord always. I will say it again: Rejoice! (Ph4:4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
