{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS Selector 활용하기 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<img alt=\"South Korea subway logo.svg\" data-file-height=\"450\" data-file-width=\"450\" decoding=\"async\" height=\"75\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/12/South_Korea_subway_logo.svg/75px-South_Korea_subway_logo.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/12/South_Korea_subway_logo.svg/113px-South_Korea_subway_logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/12/South_Korea_subway_logo.svg/150px-South_Korea_subway_logo.svg.png 2x\" width=\"75\"/>\n",
      "\n",
      "\n",
      "<img alt=\"Seoul-Metro-2004-20070722.jpg\" data-file-height=\"2100\" data-file-width=\"2800\" decoding=\"async\" height=\"225\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/2/29/Seoul-Metro-2004-20070722.jpg/300px-Seoul-Metro-2004-20070722.jpg\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/2/29/Seoul-Metro-2004-20070722.jpg/450px-Seoul-Metro-2004-20070722.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/29/Seoul-Metro-2004-20070722.jpg/600px-Seoul-Metro-2004-20070722.jpg 2x\" width=\"300\"/>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway\"\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "\n",
    "subway_image = soup.select('#mw-content-text > div.mw-parser-output > table > tbody > tr > td > a > img')\n",
    "\n",
    "print(\"\\n\")\n",
    "print(subway_image[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "subway_image2 = soup.select('tr > td > a > img')\n",
    "print(subway_image2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS Selector 활용하기 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "955\n",
      "\n",
      "\n",
      "[<a id=\"top\"></a>, <a class=\"mw-jump-link\" href=\"#mw-head\">Jump to navigation</a>, <a class=\"mw-jump-link\" href=\"#searchInput\">Jump to search</a>]\n",
      "\n",
      "\n",
      "[<a class=\"external text\" href=\"http://www.seoulmetro.co.kr/kr/board.do?menuIdx=548\" rel=\"nofollow\">\"자료실 : 알림마당&gt;자료실&gt;자료실\"</a>, <a class=\"external text\" href=\"http://www.korail.com/file/statistics/2012/2012-04.pdf\" rel=\"nofollow\">2012 Korail Statistics</a>, <a class=\"external text\" href=\"https://web.archive.org/web/20140227072212/http://www.korail.com/file/statistics/2012/2012-04.pdf\" rel=\"nofollow\">Archived</a>]\n",
      "\n",
      "\n",
      "[<div class=\"mw-body-content\" id=\"siteNotice\"><!-- CentralNotice --></div>]\n",
      "\n",
      "\n",
      "[<div class=\"mw-body-content\" id=\"siteNotice\"><!-- CentralNotice --></div>]\n",
      "\n",
      "\n",
      "[]\n",
      "\n",
      "\n",
      "[<span class=\"mw-headline\" id=\"Overview\">Overview</span>, <span class=\"mw-headline\" id=\"History\">History</span>, <span class=\"mw-headline\" id=\"Lines_and_branches\">Lines and branches</span>, <span class=\"mw-headline\" id=\"Rolling_stock\">Rolling stock</span>, <span class=\"mw-headline\" id=\"Fares_and_ticketing\">Fares and ticketing</span>, <span class=\"mw-headline\" id=\"Current_construction\">Current construction</span>, <span class=\"mw-headline\" id=\"Opening_2020\">Opening 2020</span>, <span class=\"mw-headline\" id=\"Opening_2021\">Opening 2021</span>, <span class=\"mw-headline\" id=\"Opening_2022\">Opening 2022</span>, <span class=\"mw-headline\" id=\"Opening_2023\">Opening 2023</span>, <span class=\"mw-headline\" id=\"Tentative\">Tentative</span>, <span class=\"mw-headline\" id=\"Under_planning\">Under planning</span>, <span class=\"mw-headline\" id=\"Seoul_City\">Seoul City</span>, <span class=\"mw-headline\" id=\"Incheon_City\">Incheon City</span>, <span class=\"mw-headline\" id=\"Network_map\">Network map</span>, <span class=\"mw-headline\" id=\"See_also\">See also</span>, <span class=\"mw-headline\" id=\"Notes\">Notes</span>, <span class=\"mw-headline\" id=\"References\">References</span>, <span class=\"mw-headline\" id=\"External_links\">External links</span>]\n",
      "\n",
      "\n",
      "[<span class=\"mw-headline\" id=\"Overview\">Overview</span>, <span class=\"mw-headline\" id=\"History\">History</span>, <span class=\"mw-headline\" id=\"Lines_and_branches\">Lines and branches</span>, <span class=\"mw-headline\" id=\"Rolling_stock\">Rolling stock</span>, <span class=\"mw-headline\" id=\"Fares_and_ticketing\">Fares and ticketing</span>, <span class=\"mw-headline\" id=\"Current_construction\">Current construction</span>, <span class=\"mw-headline\" id=\"Opening_2020\">Opening 2020</span>, <span class=\"mw-headline\" id=\"Opening_2021\">Opening 2021</span>, <span class=\"mw-headline\" id=\"Opening_2022\">Opening 2022</span>, <span class=\"mw-headline\" id=\"Opening_2023\">Opening 2023</span>, <span class=\"mw-headline\" id=\"Tentative\">Tentative</span>, <span class=\"mw-headline\" id=\"Under_planning\">Under planning</span>, <span class=\"mw-headline\" id=\"Seoul_City\">Seoul City</span>, <span class=\"mw-headline\" id=\"Incheon_City\">Incheon City</span>, <span class=\"mw-headline\" id=\"Network_map\">Network map</span>, <span class=\"mw-headline\" id=\"See_also\">See also</span>, <span class=\"mw-headline\" id=\"Notes\">Notes</span>, <span class=\"mw-headline\" id=\"References\">References</span>, <span class=\"mw-headline\" id=\"External_links\">External links</span>]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway\"\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "                 \n",
    "links = soup.select('a')\n",
    "print(len(links))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(links[:3])\n",
    "print(\"\\n\")\n",
    "\n",
    "external_links = soup.select('a[class=\"external text\"]')\n",
    "print(external_links[:3])\n",
    "print(\"\\n\")\n",
    "\n",
    "id_selector = soup.select('#siteNotice')\n",
    "print(id_selector)\n",
    "print(\"\\n\")\n",
    "\n",
    "id_selector2 = soup.select('div#siteNotice')\n",
    "print(id_selector2)\n",
    "print(\"\\n\")\n",
    "\n",
    "id_selector3 = soup.select('p#siteNotice')\n",
    "print(id_selector3)\n",
    "print(\"\\n\")\n",
    "\n",
    "class_selector = soup.select('.mw-headline')\n",
    "print(class_selector)\n",
    "print(\"\\n\")\n",
    "\n",
    "class_selector2 = soup.select('span.mw-headline')\n",
    "print(class_selector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구글 뉴스 클리핑하기 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'link': ['https://news.google.com/articles/CBMiK2h0dHBzOi8vd3d3LnNlZGFpbHkuY29tL05ld3NWSWV3LzFaQUdHU0pCU0rSASxodHRwczovL20uc2VkYWlseS5jb20vTmV3c1ZpZXdBbXAvMVpBR0dTSkJTSg?hl=ko&gl=KR&ceid=KR%3Ako', 'https://news.google.com/articles/CBMiMWh0dHBzOi8vd3d3LmNvZGluZ3dvcmxkbmV3cy5jb20vYXJ0aWNsZS92aWV3LzE3NTLSAQA?hl=ko&gl=KR&ceid=KR%3Ako'], 'title': [\"국민대 일자리센터, 프로그래밍 언어 '파이썬' 교육 진행 - 서울경제\", '파이썬, 데이터 과학 분야에서 인기가 많은 이유는?'], 'contents': [\"국민대 대학 일자리센터가 지난 14~15일 '파이썬(Python) 프로그래밍 교육 과정'을 진행했다.파이썬은 대표적인 프로그래밍 언어다. 파이썬을 배워두면 네트워크 ...\", '데이터 과학 업계에서 가장 많이 사용하는 프로그래밍 언어는 파이썬이다. 프로그래밍 언어의 종류는 다양하다. 그런데, 데이터 과학 업계에서는 파이썬을 제외한 ...'], 'agency': ['서울경제신문', '코딩월드뉴스'], 'date': ['2020-11-18', '2020-11-20'], 'time': ['12:45:52', '06:18:00']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://news.google.com\"\n",
    "search_url = base_url + \"/search?q=%ED%8C%8C%EC%9D%B4%EC%8D%AC&hl=ko&gl=KR&ceid=KR%3Ako\"\n",
    "resp = requests.get(search_url)\n",
    "html_src = resp.text\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "\n",
    "news_items = soup.select('div[class=\"xrnccd\"]')\n",
    "# print(len(news_items))\n",
    "# print(news_items[0])\n",
    "# print(\"\\n\")\n",
    "\n",
    "# for item in news_items[:3]:\n",
    "#     link = item.find('a', attrs={'class':'VDXfz'}).get('href')\n",
    "#     news_link = base_url + link[1:]\n",
    "#     print(news_link)\n",
    "    \n",
    "#     news_title = item.find('a', attrs={'class':'DY5T1d'}).getText()\n",
    "#     print(news_title)\n",
    "\n",
    "#     news_content = item.find('span', attrs={'class':'xBbh9'}).text\n",
    "#     print(news_content)\n",
    "\n",
    "#     news_agency = item.find('a', attrs={'class':'wEwyrc AVN2gc uQIVzc Sksgp'}).text\n",
    "#     print(news_agency)\n",
    "\n",
    "#     news_reporting = item.find('time', attrs={'class':'WW6dff uQIVzc Sksgp'})\n",
    "#     news_reporting_datetime = news_reporting.get('datetime').split('T')\n",
    "#     news_reporting_date = news_reporting_datetime[0]\n",
    "#     news_reporting_time = news_reporting_datetime[1][:-1]\n",
    "#     print(news_reporting_date, news_reporting_time)    \n",
    "#     print(\"\\n\")\n",
    "    \n",
    "def google_news_clipping(url, limit=5):\n",
    "    resp = requests.get(url)\n",
    "    html_src = resp.text\n",
    "    soup = BeautifulSoup(html_src, 'html.parser')\n",
    "    \n",
    "    news_items = soup.select('div[class=\"xrnccd\"]')\n",
    "    \n",
    "    links=[]; titles=[]; contents=[]; agencies=[]; reporting_dates=[]; reporting_times=[];\n",
    "    \n",
    "    for item in news_items[:limit]:\n",
    "        link = item.find('a', attrs={'class':'VDXfz'}).get('href')\n",
    "#         print('link',link)\n",
    "        news_link = base_url + link[1:]\n",
    "        links.append(news_link)\n",
    "        \n",
    "        news_title = item.find('a', attrs={'class':'DY5T1d'}).getText()\n",
    "        titles.append(news_title)\n",
    "    \n",
    "        news_content = item.find('span', attrs={'class':'xBbh9'}).text\n",
    "        contents.append(news_content)\n",
    "    \n",
    "        news_agency = item.find('a', attrs={'class':'wEwyrc AVN2gc uQIVzc Sksgp'}).text\n",
    "        agencies.append(news_agency)\n",
    "    \n",
    "        news_reporting = item.find('time', attrs={'class':'WW6dff uQIVzc Sksgp'})\n",
    "        news_reporting_datetime = news_reporting.get('datetime').split('T')\n",
    "        news_reporting_date = news_reporting_datetime[0]\n",
    "        news_reporting_time = news_reporting_datetime[1][:-1]\n",
    "        reporting_dates.append(news_reporting_date)\n",
    "        reporting_times.append(news_reporting_time)     \n",
    "    \n",
    "    result = {'link':links, 'title':titles, 'contents':contents, 'agency':agencies, \\\n",
    "              'date':reporting_dates, 'time':reporting_times}\n",
    "    \n",
    "    return result\n",
    "\n",
    "news = google_news_clipping(search_url, 2)\n",
    "print(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구글 뉴스 클리핑하기 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어와 조합한 URL:  https://news.google.com/search?q=%ED%8C%8C%EC%9D%B4%EC%8D%AC&hl=ko&gl=KR&ceid=KR%3Ako\n",
      "검색어를 입력하세요: 파이썬\n",
      "파이썬\n",
      "['https://news.google.com/articles/CBMiK2h0dHBzOi8vd3d3LnNlZGFpbHkuY29tL05ld3NWSWV3LzFaQUdHU0pCU0rSASxodHRwczovL20uc2VkYWlseS5jb20vTmV3c1ZpZXdBbXAvMVpBR0dTSkJTSg?hl=ko&gl=KR&ceid=KR%3Ako', 'https://news.google.com/articles/CBMiMWh0dHBzOi8vd3d3LmNvZGluZ3dvcmxkbmV3cy5jb20vYXJ0aWNsZS92aWV3LzE3NTLSAQA?hl=ko&gl=KR&ceid=KR%3Ako']\n",
      "['서울경제신문', '코딩월드뉴스']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "# keyword_input = '파이썬'\n",
    "# keyword = urllib.parse.quote(keyword_input)\n",
    "# print('파이썬 문자열을 URL 코드로 변환: ', keyword)\n",
    "\n",
    "base_url = \"https://news.google.com\"\n",
    "search_url = base_url + \"/search?q=\" + keyword + \"&hl=ko&gl=KR&ceid=KR%3Ako\"\n",
    "print('검색어와 조합한 URL: ', search_url)\n",
    "\n",
    "def google_news_clipping_keyword(keyword_input, limit=5):\n",
    "    \n",
    "#     keyword = urllib.parse.quote(keyword_input)\n",
    "    keyword = keyword_input\n",
    "    print(keyword)\n",
    "    url = base_url + \"/search?q=\" + keyword + \"&hl=ko&gl=KR&ceid=KR%3Ako\"\n",
    "    \n",
    "    resp = requests.get(url)\n",
    "    html_src = resp.text\n",
    "    soup = BeautifulSoup(html_src, 'html.parser')\n",
    "    \n",
    "    news_items = soup.select('div[class=\"xrnccd\"]')\n",
    "    links=[]; titles=[]; contents=[]; agencies=[]; reporting_dates=[]; reporting_times=[];\n",
    "    \n",
    "    for item in news_items[:limit]:\n",
    "        link = item.find('a', attrs={'class':'VDXfz'}).get('href')\n",
    "        news_link = base_url + link[1:]\n",
    "        links.append(news_link)\n",
    "        \n",
    "        news_title = item.find('a', attrs={'class':'DY5T1d'}).getText()\n",
    "        titles.append(news_title)\n",
    "    \n",
    "        news_content = item.find('span', attrs={'class':'xBbh9'}).text\n",
    "        contents.append(news_content)\n",
    "    \n",
    "        news_agency = item.find('a', attrs={'class':'wEwyrc AVN2gc uQIVzc Sksgp'}).text\n",
    "        agencies.append(news_agency)\n",
    "    \n",
    "        news_reporting = item.find('time', attrs={'class':'WW6dff uQIVzc Sksgp'})\n",
    "        news_reporting_datetime = news_reporting.get('datetime').split('T')\n",
    "        news_reporting_date = news_reporting_datetime[0]\n",
    "        news_reporting_time = news_reporting_datetime[1][:-1]\n",
    "        reporting_dates.append(news_reporting_date)\n",
    "        reporting_times.append(news_reporting_time)     \n",
    "    \n",
    "    result = {'link':links, 'title':titles, 'contents':contents, 'agency':agencies, \\\n",
    "              'date':reporting_dates, 'time':reporting_times}\n",
    "    \n",
    "    return result\n",
    "\n",
    "search_word = input(\"검색어를 입력하세요: \")\n",
    "news = google_news_clipping_keyword(search_word, 2)\n",
    "print(news['link'])\n",
    "print(news['agency'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
