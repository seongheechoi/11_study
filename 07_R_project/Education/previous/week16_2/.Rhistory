plot(hc_a, hang = -1, cex=0.7, main="complete")
# average linkage method
# check how different from complete method
hc_c <- hclust(dist_data, method = "average")
plot(hc_c, hang = -1, cex=0.7, main = "average")
# Ward's method
hc_c <- hclust(dist_data, method = "ward.D2")
plot(hc_c, hang = -1, cex=0.7, main = "Ward's method")
# preprocessing
dat1<-wages1833
dat1<-na.omit(dat1)
head(dat1, n=5)
install.packages("factoextra")
library(factoextra)
fviz_nbclust(dat1, kmeans, method = "wss")
set.seed(123)
km <- kmeans(dat1, 3, nstart = 25)
km
km <- kmeans(dat1, 3, nstart=10)
km
km <- kmeans(dat1, 3)
km
# visualize
fviz_cluster(km, data = dat1,
ellipse.type="convex",
repel = TRUE)
# compute PAM
library("cluster")
pam_out <- pam(dat1, 3)
pam_out
# freq of each cluster
table(pam_out$clustering)
# visualize
fviz_cluster(pam_out, data = dat1,
ellipse.type="convex",
repel = TRUE)
setwd("D:/R_project/Education/week14_1")
install.packages("arules")
library(arules)
dvd1 <- read.csv("dvdtrans.csv")
did1
dvd1
dvd.list <- split(dvd1$Item,dvd1$ID)
View(dvd.list)
dvd.trans<-as(dvd.list,"transactions")
dvd.trans
inspect(dvd.trans)
# summary of dvd.trans
summary(dvd.trans)
# for running dvdtras data
dvd_rule<-apriori(dvd.trans,
parameter = list(support=0.2,confidence = 0.20,minlen = 2))
dvd_rule
summary(dvd_rule)
inspect(dvd_rule)
# Bar chart for support>0.2
itemFrequencyPlot(dvd.trans,support=0.2,main="item for support>=0.2", col="green")
#association rule analysis
data("Groceries")
View(Groceries)
summary(Groceries)
# Bar chart for item with support>=5%
itemFrequencyPlot(Groceries,supp=0.05,main="item for support>=5%", col="green", cex=0.8)
# Bar chart for the top support 20 items
itemFrequencyPlot(Groceries,topN=20,main="support top 20 items", col="coral", cex=0.8)
# Association rule with support>5%, confidence>20% in minimum length 2
Grocery_rule<-apriori(data=Groceries,
parameter = list(support=0.05,
confidence = 0.20,
minlen = 2))
Grocery_rule
View(Grocery_rule)
#analyzing result
summary(Grocery_rule)
inspect(Grocery_rule)
# sorting by Lift
inspect(sort(Grocery_rule,by="lift"))
View(Groceries)
#analyzing result
summary(Grocery_rule)
inspect(Grocery_rule)
# searching association for interesting items
rule_interest<-subset(Grocery_rule, items %pin% c("yogurt","rolls/buns"))
# searching association for interesting items
rule_interest<-subset(Grocery_rule, items %in% c("yogurt","rolls/buns"))
inspect(rule_interest)
# Association rule with support>5%, confidence>20% in minimum length 2
Grocery_rule<-apriori(data=Groceries,
parameter = list(support=0.04,
confidence = 0.20,
minlen = 2))
# searching association for interesting items
rule_interest<-subset(Grocery_rule, items %in% c("yogurt","rolls/buns"))
inspect(rule_interest)
setwd("D:/R_project/Education/Week14-3")
# remiss data
re <- read.csv("remiss.csv")
View(re)
head(re)
str(re)
attach(re)
t1 <- glm(remiss~cell+smear+infil+blast+temp, data=re, family=binomial(logit))
summary(t1)
cor(re)
# logisitic regression (reduced model 1)
t2<-glm(remiss~cell+smear+li+temp, data=re,family=binomial(logit))
summary(t2)
# logisitic regression (reduced model 2)
t3<-glm(remiss~cell+li+temp, data=re,family=binomial(logit))
summary(t3)
# output data with predicted probability
dat1_pred <- cbind(re,t3$fitted.values)
write.table(dat1_pred,file="dat1_pred.csv", row.names=FALSE, sep=",", na=" ")
View(dat1_pred)
View(t3)
summary(t3)
setwd("D:/R_project/Education/week15_1")
#input data(iris)
iris<-read.csv(file="iris.csv")
attach(iris)
head(iris)
#Check correlation
cor(iris[1:4])
ir.pca <- prcomp(iris[,1:4], center=T, scale.=T)
ir.pca
summary(ir.pca)
# 2.scree plot : to choose the number of components
plot(ir.pca,type="l")
# either way to draw scree plot
screeplot(ir.pca)
# 3. calculate component=x_data%*% PCA weight
PRC<-as.matrix(iris[,1:4])%*%ir.pca$rotation
head(PRC)
# 4. classification using principal components
# make data with components
iris.pc<-cbind(as.data.frame(PRC), Species)
head(iris.pc)
# 5. support vector machine
# install.packages("e1071")
library (e1071)
m1 <- svm(Species~., data = iris.pc, kernel="linear")
summary(m1)
# predict class for all data
x<-iris.pc[, -5]
pred <- predict(m1, x)
# check accuracy between true class and predicted class
y<-iris.pc[,5]
table(pred, y)
setwd("D:/R_project/Education/week15_2")
wine <- read.csv(file="wine_aroma.csv")
attach(wine)
head(wine)
cor(wine[1:9])
wine[1:9]
wi.pca <- prcomp(wine[1:9], center=T, scale.=F)
wi.pca
summary(wi.pca)
View(wine)
View(wine)
# 2.scree plot : to choose the number of components
plot(wi.pca,type="l")
# 2.scree plot : to choose the number of components
plot(wi.pca,type="2")
PRC <- as.matrix(wine[,1:9])%*%wi.pca$rotation
head(PRC)
View(PRC)
wine.pc <- cbind(as.data.frame(PRC),Aroma)
Aroma
head(wine.pc)
fit1 <- lm(Aroma~PC1+PC2+PC3+PC4, data=wine.pc)
fit1
summary(fit1)
# regression(PC1-PC9)
fit2<-lm(Aroma~., data=wine.pc)
fit2
summary(fit2)
# Multiple regression with the raw data
fit3<-lm(Aroma ~., data=wine)
summary(fit3)
# residual diagnostic plot
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(fit2)
# residual diagnostic plot
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(fit3)
install.packages("pls")
library(pls)
setwd("D:/R_project/Education/week15-3")
# example PLS with gasoline data
data(gasoline)
help("gasoline")
attach(gasoline)
# descriptive statistics
par(mfrow=c(1,1))
hist(octane, col=3)
summary(octane)
# train and test set
gasTrain <- gasoline[1:50, ]
gasTest <- gasoline[51:60, ]
gasTrain <- gasoli
gasoline$NIR
# 1.check how many principal components
ga.pca<-prcomp(gasoline$NIR,center=T,scale.=F)
ga.pca
summary(ga.pca)
plot(ga.pca,type="l")
gas1 <- plsr(octane ~ NIR, ncomp = 10, data=gasTrain, validation ="LOO")
summary(gas1)
# 2. to choose the number of components
plot(RMSEP(gas1), legendpos = "topright", pch=46, cex=1.0, main="Cross-validation for # of LV")
# 3. Display the PLS model with LV=2
# scatterplot with true and predicted
plot(gas1, ncomp = 2, asp = 1, line = TRUE, cex=1.5,main="Measured vs Predicted", xlab="Measured" )
# Check explained variances proportion for X
explvar(gas1)
View(gas1)
# 4. predicted Y for test data
ypred <- predict(gas1, ncomp = 2, newdata = gasTest)
y<-gasoline$octane[51:60]
# check : RMSEP for test data
sqrt((sum(y-ypred)^2)/10)
# 5. compare with the one from #4 : RMSEP for test data
RMSEP(gas1, newdata = gasTest)
# check : RMSEP for test data
sqrt((sum(y-ypred)^2)/10)
setwd("D:/R_project/Education/week16_1")
# require mxnet
install.packages("https://github.com/jeremiedb/mxnet_winbin/raw/master/mxnet.zip",repos = NULL)
library(mxnet)
# If you have Error message "no package called XML or DiagrmmeR", then install
install.packages("XML")
install.packages("DiagrammeR")
library(mxnet)
# read csv file
iris<-read.csv("iris.csv")
attach(iris)
# change to numeric from character variable : Species
iris[,5] = as.numeric(iris[,5])-1
View(iris)
table(Species)
# check the data
head(iris, n=10)
# split train & test dataset
# training (n=100)/ test data(n=50)
set.seed(1000)
N<-nrow(iris)
tr.idx<-sample(1:N, size=N*2/3, replace=FALSE)
# split train data and test data
train<-data.matrix(iris[tr.idx,])
test<-data.matrix(iris[-tr.idx,])
# feature & Labels
train_feature<-train[,-5]
trainLabels<-train[,5]
test_feature<-test[,-5]
testLabels <-test[,5]
# Build nn model
# first layers
require(mxnet)
my_input = mx.symbol.Variable('data')
fc1 = mx.symbol.FullyConnected(data=my_input, num.hidden = 200, name='fc1')
relu1 = mx.symbol.Activation(data=fc1, act.type='relu', name='relu1')
# second layers
fc2 = mx.symbol.FullyConnected(data=relu1, num.hidden = 100, name='fc2')
relu2 = mx.symbol.Activation(data=fc2, act.type='relu', name='relu2')
# third layers
fc3 = mx.symbol.FullyConnected(data=relu2, num.hidden = 3, name='fc3')
# softmax
softmax = mx.symbol.SoftmaxOutput(data=fc3, name='sm')
model <- mx.model.FeedForward.create(softmax,
optimizer = "sgd",
array.batch.size=10,
num.round = 300, learning.rate=0.1,
X=train_feature, y=trainLabels, ctx=device,
eval.metric = mx.metric.accuracy,
array.layout = "rowmajor",
epoch.end.callback=mx.callback.log.train.metric(100))
# training
mx.set.seed(123)
device <- mx.cpu()
model <- mx.model.FeedForward.create(softmax,
optimizer = "sgd",
array.batch.size=10,
num.round = 300, learning.rate=0.1,
X=train_feature, y=trainLabels, ctx=device,
eval.metric = mx.metric.accuracy,
array.layout = "rowmajor",
epoch.end.callback=mx.callback.log.train.metric(100))
graph.viz(model$symbol)
# testing
predict_probs <- predict(model, test_feature, array.layout = "rowmajor")
predicted_labels <- max.col(t(predict_probs)) - 1
table(testLabels, predicted_labels)
sum(diag(table(testLabels, predicted_labels)))/length(predicted_labels)
setwd("D:/R_project/Education/week16_2")
# Load MNIST mn1
# 28*28, 1 channel images
mn1 <- read.csv("mini_mnist.csv")
set.seed(123)
N<-nrow(mn1)
# Load MNIST mn1
# 28*28, 1 channel images
mn1 <- read.csv("mini_mnist.csv")
set.seed(123)
N<-nrow(mn1)
tr.idx<-sample(1:N, size=N*2/3, replace=FALSE)
View(mn1)
View(mn1)
train_data <- data.matrix(mn1[tr.idx,])
View(train_data)
test_data <- data.matrix(mn1[-tr.idx,])
test <- t(test_data[,-1]/255)
features <- t(train_data[,-1]/255)
labels <- train_data[,1]
# lec16_2_cnn.r
# Convolutional Neural Network
# Require mxnet package
# install.packages("https://github.com/jeremiedb/mxnet_winbin/raw/master/mxnet.zip",repos = NULL)
library(mxnet)
source('D:/R_project/Education/week16_2/lec16_2_cnn.R', echo=TRUE)
# Load MNIST mn1
# 28*28, 1 channel images
mn1 <- read.csv("mini_mnist.csv")
set.seed(123)
N<-nrow(mn1)
tr.idx<-sample(1:N, size=N*2/3, replace=FALSE)
# split train data and test data
train_data<-data.matrix(mn1[tr.idx,])
test_data<-data.matrix(mn1[-tr.idx,])
test<-t(test_data[,-1]/255)
features<-t(train_data[,-1]/255)
labels<-train_data[,1]
# data preprocession
features_array <- features
dim(features_array) <- c(28,28,1,ncol(features))
test_array <- test
dim(test_array) <- c(28,28,1,ncol(test))
ncol(features)
table(labels)
# Build cnn model
# first conv layers
my_input = mx.symbol.Variable('data')
conv1 = mx.symbol.Convolution(data=my_input, kernel=c(4,4), stride=c(2,2), pad=c(1,1), num.filter = 20, name='conv1')
relu1 = mx.symbol.Activation(data=conv1, act.type='relu', name='relu1')
mp1 = mx.symbol.Pooling(data=relu1, kernel=c(2,2), stride=c(2,2), pool.type='max', name='pool1')
# second conv layers
conv2 = mx.symbol.Convolution(data=mp1, kernel=c(3,3), stride=c(2,2), pad=c(1,1), num.filter = 40, name='conv2')
relu2 = mx.symbol.Activation(data=conv2, act.type='relu', name='relu2')
mp2 = mx.symbol.Pooling(data=relu2, kernel=c(2,2), stride=c(2,2), pool.type='max', name='pool2')
# fully connected
fc1 = mx.symbol.FullyConnected(data=mp2, num.hidden = 1000, name='fc1')
relu3 = mx.symbol.Activation(data=fc1, act.type='relu', name='relu3')
fc2 = mx.symbol.FullyConnected(data=relu3, num.hidden = 3, name='fc2')
# softmax
sm = mx.symbol.SoftmaxOutput(data=fc2, name='sm')
# training
mx.set.seed(100)
device <- mx.cpu()
model <- mx.model.FeedForward.create(symbol=sm,
optimizer = "sgd",
array.batch.size=30,
num.round = 70, learning.rate=0.1,
X=features_array, y=labels, ctx=device,
eval.metric = mx.metric.accuracy,
epoch.end.callback=mx.callback.log.train.metric(100))
graph.viz(model$symbol)
# test
predict_probs <- predict(model, test_array)
predicted_labels <- max.col(t(predict_probs)) - 1
table(test_data[, 1], predicted_labels)
sum(diag(table(test_data[, 1], predicted_labels)))/length(predicted_labels)
# install packages
install.packages("xml2") # to read html
install.packages("rvest") # to use 'html_nodes'
install.packages("KoNLP") # korean natural language processing
install.packages("tm") # corpus, term-document matrix, etc.
install.packages("stringr") # to use 'str_match'
install.packages("stringr")
install.packages("stringr")
install.packages("stringr")
install.packages("stringr")
install.packages("stringr")
install.packages("wordcloud") # word cloud
# use packages
library(xml2)
library(rvest)
library(KoNLP)
library(tm)
library(stringr)
library(KoNLP)
# crawling base URL
url_base <- 'http://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=51786&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page='
url_base
# crawling base URL
url_base <- 'http://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=51786&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page='
# make a vector to contain comments
reviews <- c()
# start crawling
for(page in 1:10){
url <- paste(url_base, page, sep='') # from page 1 to 70
htxt <- read_html(url)
comment <- html_nodes(htxt, 'div')%>%html_nodes('div.input_netizen')%>%
html_nodes('div.score_result')%>%html_nodes('ul')%>%html_nodes('li')%>%
html_nodes('div.score_reple')%>%html_nodes('p') # exact location of comments
review <- html_text(comment) # extract only texts from comments
review <- repair_encoding(review, from = 'utf-8') # repair faulty encoding
review <- str_trim(review) # trim whitespace from start and end of string
reviews <- c(reviews, review) # save results
}
# extract nouns(N) and predicates(P)
ext_func <- function(doc){
doc_char <- as.character(doc)
ext1 <- paste(SimplePos09(doc_char))
ext2 <- str_match(ext1, '([A-Z가-힣]+)/[NP]')
keyword <- ext2[,2]
keyword[!is.na(keyword)]
}
# 1. term-document matrix
corp <- Corpus(VectorSource(reviews)) # generate a corpus
tdm <- TermDocumentMatrix(corp, # generate a term-document matrix
control=list(
tokenize=ext_func,
removePunctuation=T,
removeNumbers=T,
wordLengths=c(4,8)))
tdm_matrix <- as.matrix(tdm) # save as a matrix
Encoding(rownames(tdm_matrix)) <- "UTF-8" # encoding
word_count <- rowSums(tdm_matrix) # sum of term frequencies of each word
word_order <- word_count[order(word_count, decreasing=T)] # sort in descending order
doc <- as.data.frame(word_order) # save as a data frame
# 2. word cloud
library(wordcloud)
windowsFonts(font=windowsFont("맑은 고딕")) # set font
set.seed(1234)
wordcloud(words=rownames(doc), freq=doc$word_order, min.freq=2,
max.words=100, random.order=FALSE, scale=c(5,1), rot.per=0.35,
family="font", colors=brewer.pal(8,"Dark2"))
## modify and use the code right below only if there is a problem with 'rjava'
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_151')
install.packages("KoNLP") # korean natural language processing
install.packages("stringr") # to use 'str_match'
install.packages("stringr")
source('D:/R_project/Education/week16-3/lec16_3_tm.R', encoding = 'UTF-8', echo=TRUE)
source('D:/R_project/Education/week16-3/lec16_3_tm.R', encoding = 'UTF-8', echo=TRUE)
# crawling base URL
url_base <- 'http://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=51786&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page='
# make a vector to contain comments
reviews <- c()
# start crawling
for(page in 1:10){
url <- paste(url_base, page, sep='') # from page 1 to 70
htxt <- read_html(url)
comment <- html_nodes(htxt, 'div')%>%html_nodes('div.input_netizen')%>%
html_nodes('div.score_result')%>%html_nodes('ul')%>%html_nodes('li')%>%
html_nodes('div.score_reple')%>%html_nodes('p') # exact location of comments
review <- html_text(comment) # extract only texts from comments
review <- repair_encoding(review, from = 'utf-8') # repair faulty encoding
review <- str_trim(review) # trim whitespace from start and end of string
reviews <- c(reviews, review) # save results
}
# use packages
library(xml2)
library(rvest)
library(KoNLP)
library(tm)
library(stringr)
# crawling base URL
url_base <- 'http://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=51786&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page='
# make a vector to contain comments
reviews <- c()
# start crawling
for(page in 1:10){
url <- paste(url_base, page, sep='') # from page 1 to 70
htxt <- read_html(url)
comment <- html_nodes(htxt, 'div')%>%html_nodes('div.input_netizen')%>%
html_nodes('div.score_result')%>%html_nodes('ul')%>%html_nodes('li')%>%
html_nodes('div.score_reple')%>%html_nodes('p') # exact location of comments
review <- html_text(comment) # extract only texts from comments
review <- repair_encoding(review, from = 'utf-8') # repair faulty encoding
review <- str_trim(review) # trim whitespace from start and end of string
reviews <- c(reviews, review) # save results
}
# extract nouns(N) and predicates(P)
ext_func <- function(doc){
doc_char <- as.character(doc)
ext1 <- paste(SimplePos09(doc_char))
ext2 <- str_match(ext1, '([A-Z가-힣]+)/[NP]')
keyword <- ext2[,2]
keyword[!is.na(keyword)]
}
# 1. term-document matrix
corp <- Corpus(VectorSource(reviews)) # generate a corpus
tdm <- TermDocumentMatrix(corp, # generate a term-document matrix
control=list(
tokenize=ext_func,
removePunctuation=T,
removeNumbers=T,
wordLengths=c(4,8)))
tdm_matrix <- as.matrix(tdm) # save as a matrix
Encoding(rownames(tdm_matrix)) <- "UTF-8" # encoding
word_count <- rowSums(tdm_matrix) # sum of term frequencies of each word
word_order <- word_count[order(word_count, decreasing=T)] # sort in descending order
doc <- as.data.frame(word_order) # save as a data frame
# 2. word cloud
library(wordcloud)
windowsFonts(font=windowsFont("맑은 고딕")) # set font
set.seed(1234)
wordcloud(words=rownames(doc), freq=doc$word_order, min.freq=2,
max.words=100, random.order=FALSE, scale=c(5,1), rot.per=0.35,
family="font", colors=brewer.pal(8,"Dark2"))
# 3. co-occurrence matrix
# 3. co-occurrence matrix
#word_order2 <- order(word_count, decreasing=T) # sort in descending order
